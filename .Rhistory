# List of required packages
packages <- c("tidyverse", "caret", "randomForest", "xgboost", "e1071", "pROC", "rpart", "rpart.plot")
# Install any package not already installed
install.packages(setdiff(packages, installed.packages()[,"Package"]))
# Load the packages into the session
lapply(packages, library, character.only = TRUE)
# Load the dataset from CSV
data <- read_csv("healthcare-dataset-stroke-data.csv")
# Show data structure to understand types and format
glimpse(data)
# Replace "N/A" in 'bmi' column with NA and convert it to numeric
data$bmi[data$bmi == "N/A"] <- NA
data$bmi <- as.numeric(data$bmi)
# Check for missing values in each column
colSums(is.na(data))
# Impute missing BMI values using median
data$bmi[is.na(data$bmi)] <- median(data$bmi, na.rm = TRUE)
# Convert categorical columns to factors
data <- data %>%
mutate(
gender = as.factor(gender),
ever_married = as.factor(ever_married),
work_type = as.factor(work_type),
Residence_type = as.factor(Residence_type),
smoking_status = as.factor(smoking_status),
hypertension = as.factor(hypertension),
heart_disease = as.factor(heart_disease),
stroke = as.factor(stroke)
)
# Summary of numerical features
summary(select(data, age, avg_glucose_level, bmi))
# Distribution of stroke cases
table(data$stroke)
# Visualize age distribution by stroke
ggplot(data, aes(x = stroke, y = age, fill = stroke)) +
geom_boxplot() +
labs(title = "Age Distribution by Stroke Status")
# Visualize smoking status
ggplot(data, aes(x = smoking_status, fill = stroke)) +
geom_bar(position = "fill") +
labs(title = "Stroke Proportion by Smoking Status")
# Partition the data: 80% training, 20% testing
set.seed(123)
index <- createDataPartition(data$stroke, p = 0.8, list = FALSE)
train_data <- data[index, ]
test_data <- data[-index, ]
# One-hot encode categorical variables for xgboost
train_matrix <- model.matrix(stroke ~ . - id, data = train_data)[, -1]
test_matrix <- model.matrix(stroke ~ . - id, data = test_data)[, -1]
train_label <- as.numeric(as.character(train_data$stroke))
test_label <- as.numeric(as.character(test_data$stroke))
# Logistic Regression
log_model <- glm(stroke ~ . - id, data = train_data, family = binomial)
# Decision Tree
tree_model <- rpart(stroke ~ . - id, data = train_data, method = "class")
# Random Forest
rf_model <- randomForest(stroke ~ . - id, data = train_data, ntree = 100)
# XGBoost
xgb_model <- xgboost(data = train_matrix, label = train_label,
nrounds = 50, objective = "binary:logistic", verbose = 0)
# Logistic Regression Predictions
log_probs <- predict(log_model, test_data, type = "response")
log_preds <- ifelse(log_probs > 0.5, 1, 0)
confusionMatrix(as.factor(log_preds), test_data$stroke)
# Random Forest Predictions
rf_preds <- predict(rf_model, test_data)
confusionMatrix(rf_preds, test_data$stroke)
# Decision Tree Predictions
rpart.plot(tree_model)
tree_preds <- predict(tree_model, test_data, type = "class")
confusionMatrix(tree_preds, test_data$stroke)
# XGBoost Evaluation
xgb_preds <- predict(xgb_model, test_matrix)
xgb_classes <- ifelse(xgb_preds > 0.5, 1, 0)
confusionMatrix(as.factor(xgb_classes), test_data$stroke)
# ROC Curve and AUC
roc_obj <- roc(test_label, xgb_preds)
plot(roc_obj, main = "ROC Curve - XGBoost")
auc(roc_obj)
# Partition the data: 80% training, 20% testing
set.seed(123)
index <- createDataPartition(data$stroke, p = 0.8, list = FALSE)
train_data <- data[index, ]
test_data <- data[-index, ]
# One-hot encode categorical variables for xgboost
train_matrix <- model.matrix(stroke ~ . - id, data = train_data)[, -1]
test_matrix <- model.matrix(stroke ~ . - id, data = test_data)[, -1]
train_label <- as.numeric(as.character(train_data$stroke))
test_label <- as.numeric(as.character(test_data$stroke))
# Logistic Regression
log_model <- glm(stroke ~ . - id, data = train_data, family = binomial)
# Decision Tree
tree_model <- rpart(stroke ~ . - id, data = train_data, method = "class")
# Random Forest
rf_model <- randomForest(stroke ~ . - id, data = train_data, ntree = 100)
# XGBoost
xgb_model <- xgboost(data = train_matrix, label = train_label,
nrounds = 50, objective = "binary:logistic", verbose = 0)
# Logistic Regression Predictions
log_probs <- predict(log_model, test_data, type = "response")
log_preds <- ifelse(log_probs > 0.5, 1, 0)
confusionMatrix(as.factor(log_preds), test_data$stroke)
# Random Forest Predictions
rf_preds <- predict(rf_model, test_data)
confusionMatrix(rf_preds, test_data$stroke)
# Decision Tree Predictions
rpart.plot(tree_model)
tree_preds <- predict(tree_model, test_data, type = "class")
confusionMatrix(tree_preds, test_data$stroke)
# XGBoost Evaluation
xgb_preds <- predict(xgb_model, test_matrix)
xgb_classes <- ifelse(xgb_preds > 0.5, 1, 0)
confusionMatrix(as.factor(xgb_classes), test_data$stroke)
# ROC Curve and AUC
roc_obj <- roc(test_label, xgb_preds)
plot(roc_obj, main = "ROC Curve - XGBoost")
auc(roc_obj)
# Save the best model (e.g., Random Forest)
saveRDS(rf_model, file = "stroke_rf_model.rds")
# Load model for deployment
loaded_model <- readRDS("stroke_rf_model.rds")
# Predict on new data sample
predict(loaded_model, newdata = test_data[1:5, ])
# List of required packages
packages <- c("tidyverse", "caret", "randomForest", "xgboost", "e1071", "pROC", "rpart", "rpart.plot", "knitr")
# Install any package not already installed
install.packages(setdiff(packages, installed.packages()[,"Package"]))
# Load the packages into the session
lapply(packages, library, character.only = TRUE)
# Install and load required packages
packages <- c("tidyverse", "caret", "randomForest", "xgboost", "e1071", "pROC", "rpart", "rpart.plot", "knitr")
# Install missing packages
install.packages(setdiff(packages, installed.packages()[,"Package"]), dependencies = TRUE)
# Load all package
lapply(packages, library, character.only = TRUE)
# Load the data
data <- read_csv("healthcare-dataset-stroke-data.csv")
# View structure
glimpse(data)
# Display first few rows
head(data)
# Ganti "N/A" menjadi NA pada kolom BMI
data$bmi[data$bmi == "N/A"] <- NA
# Konversi BMI ke numerik
data$bmi <- as.numeric(data$bmi)
# Tampilkan jumlah missing values per kolom
colSums(is.na(data))
# Imputasi nilai NA di BMI menggunakan median
data$bmi[is.na(data$bmi)] <- median(data$bmi, na.rm = TRUE)
# Ubah tipe data untuk variabel kategorik
data <- data %>%
mutate(
gender = as.factor(gender),
ever_married = as.factor(ever_married),
work_type = as.factor(work_type),
Residence_type = as.factor(Residence_type),
smoking_status = as.factor(smoking_status),
stroke = as.factor(stroke),
hypertension = as.factor(hypertension),
heart_disease = as.factor(heart_disease)
)
# Lihat struktur data setelah cleaning
str(data)
# Ringkasan statistik data numerik
summary(select(data, age, avg_glucose_level, bmi))
# Proporsi kelas target (stroke vs non-stroke)
table(data$stroke)
prop.table(table(data$stroke))
# Visualisasi: Distribusi umur berdasarkan status stroke
ggplot(data, aes(x = stroke, y = age, fill = stroke)) +
geom_boxplot() +
labs(title = "Age Distribution by Stroke Status", y = "Age", x = "Stroke")
# Visualisasi: Status merokok dan stroke
ggplot(data, aes(x = smoking_status, fill = stroke)) +
geom_bar(position = "fill") +
labs(title = "Proportion of Stroke by Smoking Status", y = "Proportion")
# Korelasi antar variabel numerik
cor_matrix <- cor(select(data, age, avg_glucose_level, bmi))
cor_matrix
# Split data menjadi train (80%) dan test (20%)
set.seed(123)
split_index <- createDataPartition(data$stroke, p = 0.8, list = FALSE)
train_data <- data[split_index, ]
test_data <- data[-split_index, ]
# Logistic Regression
log_model <- glm(stroke ~ . - id, data = train_data, family = "binomial")
# Decision Tree
tree_model <- rpart(stroke ~ . - id, data = train_data, method = "class")
# Random Forest
rf_model <- randomForest(stroke ~ . - id, data = train_data, ntree = 100)
# XGBoost: Perlu input matrix
train_matrix <- model.matrix(stroke ~ . - id, data = train_data)[, -1]
test_matrix <- model.matrix(stroke ~ . - id, data = test_data)[, -1]
train_label <- as.numeric(as.character(train_data$stroke))
test_label <- as.numeric(as.character(test_data$stroke))
xgb_model <- xgboost(data = train_matrix, label = train_label, objective = "binary:logistic", nrounds = 50, verbose = 0)
# Logistic Regression - prediksi probabilitas
log_probs <- predict(log_model, test_data, type = "response")
log_pred <- ifelse(log_probs > 0.5, 1, 0)
confusionMatrix(as.factor(log_pred), test_data$stroke)
# ROC Curve dan AUC untuk Logistic Regression
roc_log <- roc(as.numeric(as.character(test_data$stroke)), log_probs)
auc(roc_log)
# Random Forest
rf_pred <- predict(rf_model, test_data)
confusionMatrix(rf_pred, test_data$stroke)
# Decision Tree
tree_pred <- predict(tree_model, test_data, type = "class")
confusionMatrix(tree_pred, test_data$stroke)
# XGBoost
xgb_probs <- predict(xgb_model, test_matrix)
xgb_pred <- ifelse(xgb_probs > 0.5, 1, 0)
confusionMatrix(as.factor(xgb_pred), test_data$stroke)
# AUC XGBoost
roc_xgb <- roc(test_label, xgb_probs)
auc(roc_xgb)
# Simpan model terbaik (misalnya Random Forest)
saveRDS(rf_model, "stroke_model_rf.rds")
# Load kembali model (simulasi deployment)
loaded_model <- readRDS("stroke_model_rf.rds")
# Prediksi pada data baru (5 baris pertama)
predict(loaded_model, test_data[1:5, ])
# List of required packages
packages <- c("tidyverse", "caret", "randomForest", "xgboost", "e1071", "pROC", "rpart", "rpart.plot", "knitr")
# Install any package not already installed
install.packages(setdiff(packages, installed.packages()[,"Package"]))
# Load the packages into the session
lapply(packages, library, character.only = TRUE)
# Replace "N/A" in 'bmi' column with NA and convert it to numeric
data$bmi[data$bmi == "N/A"] <- NA
data$bmi <- as.numeric(data$bmi)
# Check for missing values in each column
colSums(is.na(data))
# Impute missing BMI values using median
data$bmi[is.na(data$bmi)] <- median(data$bmi, na.rm = TRUE)
# Convert categorical columns to factors
data <- data %>%
mutate(
gender = as.factor(gender),
ever_married = as.factor(ever_married),
work_type = as.factor(work_type),
Residence_type = as.factor(Residence_type),
smoking_status = as.factor(smoking_status),
hypertension = as.factor(hypertension),
heart_disease = as.factor(heart_disease),
stroke = as.factor(stroke)
)
str(data)
# Partition the data: 80% training, 20% testing
set.seed(123)
index <- createDataPartition(data$stroke, p = 0.8, list = FALSE)
train_data <- data[index, ]
test_data <- data[-index, ]
# One-hot encode categorical variables for xgboost
train_matrix <- model.matrix(stroke ~ . - id, data = train_data)[, -1]
test_matrix <- model.matrix(stroke ~ . - id, data = test_data)[, -1]
train_label <- as.numeric(as.character(train_data$stroke))
test_label <- as.numeric(as.character(test_data$stroke))
# Logistic Regression
log_model <- glm(stroke ~ . - id, data = train_data, family = binomial)
# Decision Tree
tree_model <- rpart(stroke ~ . - id, data = train_data, method = "class")
# Random Forest
rf_model <- randomForest(stroke ~ . - id, data = train_data, ntree = 100)
# XGBoost
xgb_model <- xgboost(data = train_matrix, label = train_label,
nrounds = 50, objective = "binary:logistic", verbose = 0)
# Split data menjadi train (80%) dan test (20%)
set.seed(123)
split_index <- createDataPartition(data$stroke, p = 0.8, list = FALSE)
train_data <- data[split_index, ]
test_data <- data[-split_index, ]
# Logistic Regression
log_model <- glm(stroke ~ . - id, data = train_data, family = "binomial")
# Decision Tree
tree_model <- rpart(stroke ~ . - id, data = train_data, method = "class")
# Random Forest
rf_model <- randomForest(stroke ~ . - id, data = train_data, ntree = 100)
# XGBoost: Perlu input matrix
train_matrix <- model.matrix(stroke ~ . - id, data = train_data)[, -1]
test_matrix <- model.matrix(stroke ~ . - id, data = test_data)[, -1]
train_label <- as.numeric(as.character(train_data$stroke))
test_label <- as.numeric(as.character(test_data$stroke))
xgb_model <- xgboost(data = train_matrix, label = train_label, objective = "binary:logistic", nrounds = 50, verbose = 0)
# Logistic Regression - prediksi probabilitas
log_probs <- predict(log_model, test_data, type = "response")
log_pred <- ifelse(log_probs > 0.5, 1, 0)
confusionMatrix(as.factor(log_pred), test_data$stroke)
# ROC Curve dan AUC untuk Logistic Regression
roc_log <- roc(as.numeric(as.character(test_data$stroke)), log_probs)
auc(roc_log)
# Random Forest
rf_pred <- predict(rf_model, test_data)
confusionMatrix(rf_pred, test_data$stroke)
# Decision Tree
tree_pred <- predict(tree_model, test_data, type = "class")
confusionMatrix(tree_pred, test_data$stroke)
# XGBoost
xgb_probs <- predict(xgb_model, test_matrix)
xgb_pred <- ifelse(xgb_probs > 0.5, 1, 0)
confusionMatrix(as.factor(xgb_pred), test_data$stroke)
# AUC XGBoost
roc_xgb <- roc(test_label, xgb_probs)
auc(roc_xgb)
